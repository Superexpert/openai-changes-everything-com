---
layout: episode
episode_id: "will-ai-kill-us-all"
---

## AI Alignment and Existential Risk: A Conversation with David Manheim

### Understanding the AI Alignment Problem
In this episode of the *Move 37 Podcast* podcast, host Stephen Walther speaks with David Manheim, an expert in existential risk and AI alignment. David discusses why building AI systems smarter than humans without proper safeguards can lead to catastrophic outcomes, highlighting the importance of safe and aligned AI development.

### Who is David Manheim?
David Manheim is a renowned researcher and policy advisor focusing on mitigating large-scale risks to humanity, specifically in biosecurity and artificial intelligence. He was an original "superforecaster" with the Good Judgment Project and currently leads policy and research at ALTER (Association for Long-term Existence and Resilience).

### Key Takeaways from the Discussion

#### Existential Risks from AI
- AI alignment problems arise when systems smarter than humans have misaligned goals, potentially leading to catastrophic outcomes.
- Manheim argues that the default trajectory of creating advanced AI without adequate alignment measures is likely dangerous, and serious caution is warranted.

#### Predicting AI Risks
- Forecasting existential AI risks is complex due to high uncertainty and rapid technological progress.
- Effective risk assessment involves careful modeling of uncertainties and identifying specific "failure modes" or scenarios where AI could become dangerous.

#### Importance of Cautious Optimism
- Manheim emphasizes cautious optimism, arguing against extremes of doom or unregulated acceleration in AI development.
- He believes humanity has a good chance to manage these risks responsibly if there's a global consensus and proactive regulatory frameworks.

#### Superforecasting and Risk Management
- "Superforecasters" possess specialized skills allowing them to predict future scenarios more accurately than typical experts, focusing on consistent updates, humility, and openness to new information.
- Manheim discusses how leaders can benefit from these forecasting techniques by balancing decisiveness with adaptive flexibility.

### AI Risk Analogies
- **Nick Bostrom’s Urn Analogy:** Future technologies are like drawing balls from an urn, some potentially catastrophic (black balls), highlighting the importance of cautious exploration.
- **Explore-Exploit Robot Analogy:** Balancing between exploiting known beneficial technologies and carefully exploring new ones to avoid catastrophic outcomes.

### How Should We Respond to AI Risks?
- Invest in AI safety research, such as provably safe AI systems and rigorous testing frameworks.
- Encourage global governance and regulation to ensure technology develops safely and ethically.

### Resources for Further Exploration
- AI Safety Institutes (UK and US)
- Stuart Russell’s research group on AI alignment
- Arya's Guaranteed Safe AI Initiative

### Final Thoughts
David Manheim remains optimistic about humanity's capacity to address and manage AI risks, stressing the need for greater caution, thoughtful regulation, and international cooperation to navigate the future safely.